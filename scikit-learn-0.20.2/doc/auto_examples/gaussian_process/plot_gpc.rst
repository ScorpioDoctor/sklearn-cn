.. note::
    :class: sphx-glr-download-link-note

    Click :ref:`here <sphx_glr_download_auto_examples_gaussian_process_plot_gpc.py>` to download the full example code
.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_gaussian_process_plot_gpc.py:


====================================================================
使用高斯过程分类器(GPC)进行概率性预测
====================================================================

这个例子展示了具有不同超参数选项的RBF内核的GPC预测概率。 
第一幅图显示的是 具有任意选择的超参数的GPC的预测概率 以及 具有与最大LML对应的超参数的GPC的预测概率。

虽然通过优化LML选择的超参数具有相当大的LML，但是依据测试数据上的对数损失，它们的表现更差。 
该图显示， 这是因为它们在类边界表现出类概率的急剧变化(这是好的表现)， 
但在远离类边界的地方 其预测概率却接近0.5（这是坏的表现） 
这种不良影响是由于GPC内部使用了拉普拉斯近似(Laplace approximation)。

第二幅图显示了 针对内核超参数的不同选择所对应的LML（对数边缘似然），
突出了在第一幅图中使用的通过黑点（训练集）选择的两个超参数。




.. rst-class:: sphx-glr-horizontal


    *

      .. image:: /auto_examples/gaussian_process/images/sphx_glr_plot_gpc_000.png
            :class: sphx-glr-multi-img

    *

      .. image:: /auto_examples/gaussian_process/images/sphx_glr_plot_gpc_001.png
            :class: sphx-glr-multi-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Log Marginal Likelihood (initial): -17.598
    Log Marginal Likelihood (optimized): -3.875
    Accuracy: 1.000 (initial) 1.000 (optimized)
    Log-loss: 0.214 (initial) 0.319 (optimized)




|


.. code-block:: python

    print(__doc__)

    # Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
    # 翻译者：http://www.studyai.com/antares
    # License: BSD 3 clause

    import numpy as np

    from matplotlib import pyplot as plt

    from sklearn.metrics.classification import accuracy_score, log_loss
    from sklearn.gaussian_process import GaussianProcessClassifier
    from sklearn.gaussian_process.kernels import RBF


    # Generate data
    train_size = 50
    rng = np.random.RandomState(0)
    X = rng.uniform(0, 5, 100)[:, np.newaxis]
    y = np.array(X[:, 0] > 2.5, dtype=int)

    # Specify Gaussian Processes with fixed and optimized hyperparameters
    gp_fix = GaussianProcessClassifier(kernel=1.0 * RBF(length_scale=1.0),
                                       optimizer=None)
    gp_fix.fit(X[:train_size], y[:train_size])

    gp_opt = GaussianProcessClassifier(kernel=1.0 * RBF(length_scale=1.0))
    gp_opt.fit(X[:train_size], y[:train_size])

    print("Log Marginal Likelihood (initial): %.3f"
          % gp_fix.log_marginal_likelihood(gp_fix.kernel_.theta))
    print("Log Marginal Likelihood (optimized): %.3f"
          % gp_opt.log_marginal_likelihood(gp_opt.kernel_.theta))

    print("Accuracy: %.3f (initial) %.3f (optimized)"
          % (accuracy_score(y[:train_size], gp_fix.predict(X[:train_size])),
             accuracy_score(y[:train_size], gp_opt.predict(X[:train_size]))))
    print("Log-loss: %.3f (initial) %.3f (optimized)"
          % (log_loss(y[:train_size], gp_fix.predict_proba(X[:train_size])[:, 1]),
             log_loss(y[:train_size], gp_opt.predict_proba(X[:train_size])[:, 1])))


    # Plot posteriors
    plt.figure(0)
    plt.scatter(X[:train_size, 0], y[:train_size], c='k', label="Train data",
                edgecolors=(0, 0, 0))
    plt.scatter(X[train_size:, 0], y[train_size:], c='g', label="Test data",
                edgecolors=(0, 0, 0))
    X_ = np.linspace(0, 5, 100)
    plt.plot(X_, gp_fix.predict_proba(X_[:, np.newaxis])[:, 1], 'r',
             label="Initial kernel: %s" % gp_fix.kernel_)
    plt.plot(X_, gp_opt.predict_proba(X_[:, np.newaxis])[:, 1], 'b',
             label="Optimized kernel: %s" % gp_opt.kernel_)
    plt.xlabel("Feature")
    plt.ylabel("Class 1 probability")
    plt.xlim(0, 5)
    plt.ylim(-0.25, 1.5)
    plt.legend(loc="best")

    # Plot LML landscape
    plt.figure(1)
    theta0 = np.logspace(0, 8, 30)
    theta1 = np.logspace(-1, 1, 29)
    Theta0, Theta1 = np.meshgrid(theta0, theta1)
    LML = [[gp_opt.log_marginal_likelihood(np.log([Theta0[i, j], Theta1[i, j]]))
            for i in range(Theta0.shape[0])] for j in range(Theta0.shape[1])]
    LML = np.array(LML).T
    plt.plot(np.exp(gp_fix.kernel_.theta)[0], np.exp(gp_fix.kernel_.theta)[1],
             'ko', zorder=10)
    plt.plot(np.exp(gp_opt.kernel_.theta)[0], np.exp(gp_opt.kernel_.theta)[1],
             'ko', zorder=10)
    plt.pcolor(Theta0, Theta1, LML)
    plt.xscale("log")
    plt.yscale("log")
    plt.colorbar()
    plt.xlabel("Magnitude")
    plt.ylabel("Length-scale")
    plt.title("Log-marginal-likelihood")

    plt.show()

**Total running time of the script:** ( 0 minutes  3.205 seconds)


.. _sphx_glr_download_auto_examples_gaussian_process_plot_gpc.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download

     :download:`Download Python source code: plot_gpc.py <plot_gpc.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: plot_gpc.ipynb <plot_gpc.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.readthedocs.io>`_
