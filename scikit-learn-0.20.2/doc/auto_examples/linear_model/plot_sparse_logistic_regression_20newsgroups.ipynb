{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# \u5728newgroups20\u6570\u636e\u96c6\u4e0a\u7684\u591a\u7c7b\u7a00\u758flogisitic\u56de\u5f52\n\n\n\u5728newgroups20\u6570\u636e\u96c6\u7684\u6587\u6863\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0c\u6bd4\u8f83 multinomial logistic L1 \u56de\u5f52\u7b97\u6cd5\n\u548c one-versus-rest L1 logistic \u56de\u5f52\u7b97\u6cd5 \u3002\n\nMultinomial logistic regression \u4ea7\u751f\u66f4\u51c6\u786e\u7684\u7ed3\u679c\u800c\u4e14\u5728\u8f83\u5927\u89c4\u6a21\u7684\u6570\u636e\u96c6\u4e0a\u53ef\u4ee5\u8bad\u7ec3\u7684\u66f4\u5feb\u3002\n\n\u8fd9\u91cc\u6211\u4eec\u4f7f\u7528 l1 sparsity \u6765\u628a\u90a3\u4e9b \u4e0d\u63d0\u4f9b\u4fe1\u606f\u7684\u7279\u5f81\u7684\u6743\u91cd \u88c1\u526a\u4e3a0\u3002\n\u5982\u679c\u6211\u4eec\u7684\u76ee\u7684\u662f\u62bd\u53d6\u6bcf\u4e2a\u7c7b\u7684\u5f3a\u8fa8\u522b\u6027\u8bcd\u6c47\uff0c\u8fd9\u4e48\u505a\u662f\u597d\u7684\u3002 \n\u5982\u679c\u6211\u4eec\u7684\u76ee\u6807\u662f\u83b7\u5f97\u6700\u4f73\u7684\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u6700\u597d\u662f\u4f7f\u7528\u65e0\u7a00\u758f\u8bf1\u5bfc\u6027\u7684L2\u60e9\u7f5a\u3002\n\n\u5728\u8f93\u5165\u7279\u5f81\u7684\u7a00\u758f\u5b50\u96c6\u4e0a\u505a\u9884\u6d4b\u7684\u4e00\u4e2a\u66f4\u4e3a\u4f20\u7edf\u7684(\u5e76\u4e14\u66f4\u597d\u7684)\u65b9\u6cd5\u662f \u4f7f\u7528\u5355\u53d8\u91cf\u7279\u5f81\u9009\u62e9\u518d\u7d27\u8ddf\u4e00\u4e2a\n\u4f20\u7edf\u7684 (l2-penalised) Logistic\u56de\u5f52\u6a21\u578b\u3002\n\n\u7ffb\u8bd1\u8005\uff1a Antares@studyai.com\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import fetch_20newsgroups_vectorized\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nprint(__doc__)\n\nt0 = time.clock()\n\n# \u6211\u4eec\u4f7f\u7528 SAGA solver\nsolver = 'saga'\n\n# \u51cf\u5c0f\u6837\u672c\u91cf\u53ef\u4ee5\u8dd1\u5f97\u66f4\u5feb\nn_samples = 10000\n\n# Memorized fetch_rcv1 for faster access\ndataset = fetch_20newsgroups_vectorized('all')\nX = dataset.data\ny = dataset.target\nX = X[:n_samples]\ny = y[:n_samples]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    random_state=42,\n                                                    stratify=y,\n                                                    test_size=0.1)\ntrain_samples, n_features = X_train.shape\nn_classes = np.unique(y).shape[0]\n\nprint('Dataset 20newsgroup, train_samples=%i, n_features=%i, n_classes=%i'\n      % (train_samples, n_features, n_classes))\n\nmodels = {'ovr': {'name': 'One versus Rest', 'iters': [1, 3]},\n          'multinomial': {'name': 'Multinomial', 'iters': [1, 3, 7]}}\n\nfor model in models:\n    # \u6dfb\u52a0\u4e00\u4e2a\u521d\u59cb\u51c6\u786e\u7387(\u57fa\u4e8e\u968f\u673a\u731c\u6d4b)\u7528\u4e8e\u7ed8\u56fe\n    accuracies = [1 / n_classes]\n    times = [0]\n    densities = [1]\n\n    model_params = models[model]\n\n    # Small number of epochs for fast runtime\n    for this_max_iter in model_params['iters']:\n        print('[model=%s, solver=%s] Number of epochs: %s' %\n              (model_params['name'], solver, this_max_iter))\n        lr = LogisticRegression(solver=solver,\n                                multi_class=model,\n                                C=1,\n                                penalty='l1',\n                                fit_intercept=True,\n                                max_iter=this_max_iter,\n                                random_state=42,\n                                )\n        t1 = time.clock()\n        lr.fit(X_train, y_train)\n        train_time = time.clock() - t1\n\n        y_pred = lr.predict(X_test)\n        accuracy = np.sum(y_pred == y_test) / y_test.shape[0]\n        density = np.mean(lr.coef_ != 0, axis=1) * 100\n        accuracies.append(accuracy)\n        densities.append(density)\n        times.append(train_time)\n    models[model]['times'] = times\n    models[model]['densities'] = densities\n    models[model]['accuracies'] = accuracies\n    print('Test accuracy for model %s: %.4f' % (model, accuracies[-1]))\n    print('%% non-zero coefficients for model %s, '\n          'per class:\\n %s' % (model, densities[-1]))\n    print('Run time (%i epochs) for model %s:'\n          '%.2f' % (model_params['iters'][-1], model, times[-1]))\n\nfig = plt.figure()\nax = fig.add_subplot(111)\n\nfor model in models:\n    name = models[model]['name']\n    times = models[model]['times']\n    accuracies = models[model]['accuracies']\n    ax.plot(times, accuracies, marker='o',\n            label='Model: %s' % name)\n    ax.set_xlabel('Train time (s)')\n    ax.set_ylabel('Test accuracy')\nax.legend()\nfig.suptitle('Multinomial vs One-vs-Rest Logistic L1\\n'\n             'Dataset %s' % '20newsgroups')\nfig.tight_layout()\nfig.subplots_adjust(top=0.85)\nrun_time = time.clock() - t0\nprint('Example run in %.3f s' % run_time)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}