{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n==================================================\n\u81ea\u52a8\u5173\u8054\u786e\u5b9a \u56de\u5f52 (ARD)\n==================================================\n\n\u4f7f\u7528\u81ea\u52a8\u5173\u8054\u786e\u5b9a\u7406\u8bba\u62df\u5408\u56de\u5f52\u6a21\u578b\u3002\n\n\u8bf7\u770b \u81ea\u52a8\u5173\u8054\u786e\u5b9a(`automatic_correlation_determination`) \u7528\u4e8e\u56de\u5f52\u7684\u66f4\u591a\u5185\u5bb9\u3002\n\n\u4e0eOLS(\u666e\u901a\u6700\u5c0f\u4e8c\u4e58)\u4f30\u8ba1\u5668\u76f8\u6bd4\uff0c\u7cfb\u6570\u6743\u503c\u7565\u5411\u96f6\u6f02\u79fb\uff0c\u4ece\u800c\u4f7f\u5176\u7a33\u5b9a\u3002\n\n\u4f30\u8ba1\u51fa\u7684\u6743\u91cd\u7684\u76f4\u65b9\u56fe\u662f\u975e\u5e38\u5c16\u5cf0\u7684(very peaked)\uff0c\u56e0\u4e3a\u5728\u6743\u91cd\u4e0a\u9690\u542b\u4e86\u4ece\u7a00\u758f\u8bf1\u5bfc\u7684\u5148\u9a8c(sparsity-inducing prior)\u3002\n\n\u6a21\u578b\u7684\u4f30\u8ba1\u662f\u901a\u8fc7 \u8fed\u4ee3\u5730\u6700\u5927\u5316 \u89c2\u6d4b\u503c\u7684\u8fb9\u9645\u5bf9\u6570\u4f3c\u7136(marginal log-likelihood of the observations)\u6765\u5b9e\u73b0\u7684\u3002\n\n\u6211\u4eec\u8fd8\u7528\u591a\u9879\u5f0f\u7279\u5f81\u5c55\u5f00(polynomial feature expansion)\u7ed8\u5236\u4e86\u4e00\u7ef4\u56de\u5f52\u60c5\u5f62\u4e0b\u7684ARD\u7684\u9884\u6d4b\u548c\u4e0d\u786e\u5b9a\u6027\u56fe\u3002 \n\u6ce8\u610f\u5230\uff0c\u4e0d\u786e\u5b9a\u6027\u503c\u5728\u56fe\u7684\u53f3\u8fb9\u5f00\u59cb\u4e0a\u5347\u3002\u8fd9\u662f\u56e0\u4e3a\u8fd9\u4e9b\u6d4b\u8bd5\u6837\u672c\u8d85\u51fa\u4e86\u8bad\u7ec3\u6837\u672c\u7684\u8303\u56f4\u3002\n\n\u7ffb\u8bd1\u8005\uff1astudyai.com\u7684Antares\u535a\u58eb\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nfrom sklearn.linear_model import ARDRegression, LinearRegression\n\n# #############################################################################\n# \u7528\u9ad8\u65af\u6743\u503c(Gaussian weights)\u751f\u6210\u6a21\u62df\u6570\u636e\n\n# \u6837\u672c\u96c6\u7684\u53c2\u6570(\u6837\u672c\u6570\u91cf\uff0c\u7279\u5f81\u6570\u91cf)\nnp.random.seed(0)\nn_samples, n_features = 100, 100\n# \u4ea7\u751f\u670d\u4ece\u9ad8\u65af\u5206\u5e03\u7684\u6570\u636e\nX = np.random.randn(n_samples, n_features)\n# Create weights with a precision lambda_ of 4.\nlambda_ = 4.\nw = np.zeros(n_features)\n# \u53ea\u4fdd\u7559 10 \u4e2a\u611f\u5174\u8da3\u7684\u6743\u91cd\nrelevant_features = np.random.randint(0, n_features, 10)\nfor i in relevant_features:\n    w[i] = stats.norm.rvs(loc=0, scale=1. / np.sqrt(lambda_))\n# Create noise with a precision alpha of 50.\nalpha_ = 50.\nnoise = stats.norm.rvs(loc=0, scale=1. / np.sqrt(alpha_), size=n_samples)\n# \u4ea7\u751f \u76ee\u6807\u503c(target)\ny = np.dot(X, w) + noise\n\n# #############################################################################\n# \u62df\u5408 ARD Regression \u6a21\u578b\nclf = ARDRegression(compute_score=True)\nclf.fit(X, y)\n# \u62df\u5408 OLS Regression \u6a21\u578b\nols = LinearRegression()\nols.fit(X, y)\n\n# #############################################################################\n# Plot the true weights, the estimated weights, the histogram of the\n# weights, and predictions with standard deviations\nplt.figure(figsize=(6, 5))\nplt.title(\"Weights of the model\")\nplt.plot(clf.coef_, color='darkblue', linestyle='-', linewidth=2,\n         label=\"ARD estimate\")\nplt.plot(ols.coef_, color='yellowgreen', linestyle=':', linewidth=2,\n         label=\"OLS estimate\")\nplt.plot(w, color='orange', linestyle='-', linewidth=2, label=\"Ground truth\")\nplt.xlabel(\"Features\")\nplt.ylabel(\"Values of the weights\")\nplt.legend(loc=1)\n\nplt.figure(figsize=(6, 5))\nplt.title(\"Histogram of the weights\")\nplt.hist(clf.coef_, bins=n_features, color='navy', log=True)\nplt.scatter(clf.coef_[relevant_features], np.full(len(relevant_features), 5.),\n            color='gold', marker='o', label=\"Relevant features\")\nplt.ylabel(\"Features\")\nplt.xlabel(\"Values of the weights\")\nplt.legend(loc=1)\n\nplt.figure(figsize=(6, 5))\nplt.title(\"Marginal log-likelihood\")\nplt.plot(clf.scores_, color='navy', linewidth=2)\nplt.ylabel(\"Score\")\nplt.xlabel(\"Iterations\")\n\n\n# Plotting some predictions for polynomial regression\ndef f(x, noise_amount):\n    y = np.sqrt(x) * np.sin(x)\n    noise = np.random.normal(0, 1, len(x))\n    return y + noise_amount * noise\n\n\ndegree = 10\nX = np.linspace(0, 10, 100)\ny = f(X, noise_amount=1)\nclf_poly = ARDRegression(threshold_lambda=1e5)\nclf_poly.fit(np.vander(X, degree), y)\n\nX_plot = np.linspace(0, 11, 25)\ny_plot = f(X_plot, noise_amount=0)\ny_mean, y_std = clf_poly.predict(np.vander(X_plot, degree), return_std=True)\nplt.figure(figsize=(6, 5))\nplt.errorbar(X_plot, y_mean, y_std, color='navy',\n             label=\"Polynomial ARD\", linewidth=2)\nplt.plot(X_plot, y_plot, color='gold', linewidth=2,\n         label=\"Ground Truth\")\nplt.ylabel(\"Output y\")\nplt.xlabel(\"Feature X\")\nplt.legend(loc=\"lower left\")\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}