.. note::
    :class: sphx-glr-download-link-note

    Click :ref:`here <sphx_glr_download_auto_examples_linear_model_plot_huber_vs_ridge.py>` to download the full example code
.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_linear_model_plot_huber_vs_ridge.py:


=======================================================
在具有强离群点的数据集上的对比：HuberRegressor vs Ridge
=======================================================

在具有强离群点的数据集上拟合 Ridge 和 HuberRegressor.

该例表明，Ridge的预测受数据集中的离群值的影响很大。
由于Huber regressor使用了线性损失，因此该模型受离群值的影响较小。
随着Huber回归方程参数epsilon的增大，决策函数接近于Ridge。




.. image:: /auto_examples/linear_model/images/sphx_glr_plot_huber_vs_ridge_001.png
    :class: sphx-glr-single-img





.. code-block:: python


    # Authors: Manoj Kumar mks542@nyu.edu
    # License: BSD 3 clause
    # 翻译者：www.studyai.com/antares

    print(__doc__)

    import numpy as np
    import matplotlib.pyplot as plt

    from sklearn.datasets import make_regression
    from sklearn.linear_model import HuberRegressor, Ridge

    # 产生迷你数据集
    rng = np.random.RandomState(0)
    X, y = make_regression(n_samples=20, n_features=1, random_state=0, noise=4.0,
                           bias=100.0)

    # 为数据集添加4个强大的 离群点
    X_outliers = rng.normal(0, 0.5, size=(4, 1))
    y_outliers = rng.normal(0, 2.0, size=4)
    X_outliers[:2, :] += X.max() + X.mean() / 4.
    X_outliers[2:, :] += X.min() - X.mean() / 4.
    y_outliers[:2] += y.min() - y.mean() / 4.
    y_outliers[2:] += y.max() + y.mean() / 4.
    X = np.vstack((X, X_outliers))
    y = np.concatenate((y, y_outliers))
    plt.plot(X, y, 'b.')

    # Fit the huber regressor over a series of epsilon values.
    colors = ['r-', 'b-', 'y-', 'm-']

    x = np.linspace(X.min(), X.max(), 7)
    epsilon_values = [1.35, 1.5, 1.75, 1.9]
    for k, epsilon in enumerate(epsilon_values):
        huber = HuberRegressor(fit_intercept=True, alpha=0.0, max_iter=100,
                               epsilon=epsilon)
        huber.fit(X, y)
        coef_ = huber.coef_ * x + huber.intercept_
        plt.plot(x, coef_, colors[k], label="huber loss, %s" % epsilon)

    # 拟合一个 岭回归 模型与 huber 回归 做对比.
    ridge = Ridge(fit_intercept=True, alpha=0.0, random_state=0, normalize=True)
    ridge.fit(X, y)
    coef_ridge = ridge.coef_
    coef_ = ridge.coef_ * x + ridge.intercept_
    plt.plot(x, coef_, 'g-', label="ridge regression")

    plt.title("Comparison of HuberRegressor vs Ridge")
    plt.xlabel("X")
    plt.ylabel("y")
    plt.legend(loc=0)
    plt.show()

**Total running time of the script:** ( 0 minutes  0.102 seconds)


.. _sphx_glr_download_auto_examples_linear_model_plot_huber_vs_ridge.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download

     :download:`Download Python source code: plot_huber_vs_ridge.py <plot_huber_vs_ridge.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: plot_huber_vs_ridge.ipynb <plot_huber_vs_ridge.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.readthedocs.io>`_
