{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n=======================================================\n\u5728\u5177\u6709\u5f3a\u79bb\u7fa4\u70b9\u7684\u6570\u636e\u96c6\u4e0a\u7684\u5bf9\u6bd4\uff1aHuberRegressor vs Ridge\n=======================================================\n\n\u5728\u5177\u6709\u5f3a\u79bb\u7fa4\u70b9\u7684\u6570\u636e\u96c6\u4e0a\u62df\u5408 Ridge \u548c HuberRegressor.\n\n\u8be5\u4f8b\u8868\u660e\uff0cRidge\u7684\u9884\u6d4b\u53d7\u6570\u636e\u96c6\u4e2d\u7684\u79bb\u7fa4\u503c\u7684\u5f71\u54cd\u5f88\u5927\u3002\n\u7531\u4e8eHuber regressor\u4f7f\u7528\u4e86\u7ebf\u6027\u635f\u5931\uff0c\u56e0\u6b64\u8be5\u6a21\u578b\u53d7\u79bb\u7fa4\u503c\u7684\u5f71\u54cd\u8f83\u5c0f\u3002\n\u968f\u7740Huber\u56de\u5f52\u65b9\u7a0b\u53c2\u6570epsilon\u7684\u589e\u5927\uff0c\u51b3\u7b56\u51fd\u6570\u63a5\u8fd1\u4e8eRidge\u3002\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Manoj Kumar mks542@nyu.edu\n# License: BSD 3 clause\n# \u7ffb\u8bd1\u8005\uff1awww.studyai.com/antares\n\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import HuberRegressor, Ridge\n\n# \u4ea7\u751f\u8ff7\u4f60\u6570\u636e\u96c6\nrng = np.random.RandomState(0)\nX, y = make_regression(n_samples=20, n_features=1, random_state=0, noise=4.0,\n                       bias=100.0)\n\n# \u4e3a\u6570\u636e\u96c6\u6dfb\u52a04\u4e2a\u5f3a\u5927\u7684 \u79bb\u7fa4\u70b9\nX_outliers = rng.normal(0, 0.5, size=(4, 1))\ny_outliers = rng.normal(0, 2.0, size=4)\nX_outliers[:2, :] += X.max() + X.mean() / 4.\nX_outliers[2:, :] += X.min() - X.mean() / 4.\ny_outliers[:2] += y.min() - y.mean() / 4.\ny_outliers[2:] += y.max() + y.mean() / 4.\nX = np.vstack((X, X_outliers))\ny = np.concatenate((y, y_outliers))\nplt.plot(X, y, 'b.')\n\n# Fit the huber regressor over a series of epsilon values.\ncolors = ['r-', 'b-', 'y-', 'm-']\n\nx = np.linspace(X.min(), X.max(), 7)\nepsilon_values = [1.35, 1.5, 1.75, 1.9]\nfor k, epsilon in enumerate(epsilon_values):\n    huber = HuberRegressor(fit_intercept=True, alpha=0.0, max_iter=100,\n                           epsilon=epsilon)\n    huber.fit(X, y)\n    coef_ = huber.coef_ * x + huber.intercept_\n    plt.plot(x, coef_, colors[k], label=\"huber loss, %s\" % epsilon)\n\n# \u62df\u5408\u4e00\u4e2a \u5cad\u56de\u5f52 \u6a21\u578b\u4e0e huber \u56de\u5f52 \u505a\u5bf9\u6bd4.\nridge = Ridge(fit_intercept=True, alpha=0.0, random_state=0, normalize=True)\nridge.fit(X, y)\ncoef_ridge = ridge.coef_\ncoef_ = ridge.coef_ * x + ridge.intercept_\nplt.plot(x, coef_, 'g-', label=\"ridge regression\")\n\nplt.title(\"Comparison of HuberRegressor vs Ridge\")\nplt.xlabel(\"X\")\nplt.ylabel(\"y\")\nplt.legend(loc=0)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}