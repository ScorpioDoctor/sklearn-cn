{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# \u8d1d\u53f6\u65af\u5cad\u56de\u5f52\n\n\n\u5728\u4e00\u4e2a\u4eba\u5de5\u5408\u6210\u7684\u6570\u636e\u96c6\u4e0a\u8ba1\u7b97\u8d1d\u53f6\u65af\u5cad\u56de\u5f52(Bayesian Ridge Regression)\u3002\n\n\u8bf7\u53c2\u8003 `bayesian_ridge_regression` \u83b7\u5f97\u5173\u4e8e\u6b64\u56de\u5f52\u7b97\u6cd5\u7684\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\u3002\n\n\u4e0eOLS(\u666e\u901a\u6700\u5c0f\u4e8c\u4e58)\u4f30\u8ba1\u5668\u76f8\u6bd4\uff0c\u7cfb\u6570\u6743\u503c\u7565\u5411\u96f6\u6f02\u79fb\uff0c\u4ece\u800c\u4f7f\u5176\u7a33\u5b9a\u3002\n\n\u7531\u4e8e\u6743\u91cd\u4e0a\u7684\u5148\u9a8c\u662f\u9ad8\u65af\u5148\u9a8c(Gaussian prior), \u6240\u4ee5\u4f30\u8ba1\u51fa\u7684\u6743\u91cd\u7684\u76f4\u65b9\u56fe\u662f\u4e2a\u7c7b\u4f3c\u9ad8\u65af\u5206\u5e03\u7684\u76f4\u65b9\u56fe\u3002\n\n\u6a21\u578b\u7684\u4f30\u8ba1\u662f\u901a\u8fc7 \u8fed\u4ee3\u5730\u6700\u5927\u5316 \u89c2\u6d4b\u503c\u7684\u8fb9\u9645\u5bf9\u6570\u4f3c\u7136(marginal log-likelihood of the observations)\u6765\u5b9e\u73b0\u7684\u3002\n\n\u6211\u4eec\u8fd8\u7528\u591a\u9879\u5f0f\u7279\u5f81\u5c55\u5f00(polynomial feature expansion)\u7ed8\u5236\u4e86\u4e00\u7ef4\u56de\u5f52\u60c5\u5f62\u4e0b\u7684\u8d1d\u53f6\u65af\u5cad\u56de\u5f52\u7684\u9884\u6d4b\u548c\u4e0d\u786e\u5b9a\u6027\u56fe\u3002\n\u6ce8\u610f\u5230\uff0c\u4e0d\u786e\u5b9a\u6027\u503c\u5728\u56fe\u7684\u53f3\u8fb9\u5f00\u59cb\u4e0a\u5347\u3002\u8fd9\u662f\u56e0\u4e3a\u8fd9\u4e9b\u6d4b\u8bd5\u6837\u672c\u8d85\u51fa\u4e86\u8bad\u7ec3\u6837\u672c\u7684\u8303\u56f4\u3002\n\n\u7ffb\u8bd1\u8005\uff1astudyai.com\u7684Antares\u535a\u58eb\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nfrom sklearn.linear_model import BayesianRidge, LinearRegression\n\n# #############################################################################\n# \u7528\u9ad8\u65af\u6743\u503c(Gaussian weights)\u751f\u6210\u6a21\u62df\u6570\u636e\nnp.random.seed(0)\nn_samples, n_features = 100, 100\nX = np.random.randn(n_samples, n_features)  # \u4ea7\u751f\u670d\u4ece\u9ad8\u65af\u5206\u5e03\u7684\u6570\u636e\n# \u7528\u7b49\u4e8e 4 \u7684 precision lambda_ \u4ea7\u751f\u6743\u91cd\u3002\nlambda_ = 4.\nw = np.zeros(n_features)\n# \u53ea\u4fdd\u7559 10 \u4e2a\u611f\u5174\u8da3\u7684\u6743\u91cd\nrelevant_features = np.random.randint(0, n_features, 10)\nfor i in relevant_features:\n    w[i] = stats.norm.rvs(loc=0, scale=1. / np.sqrt(lambda_))\n# \u7528\u53d6\u503c\u4e3a 50 \u7684 precision alpha_ \u4ea7\u751f\u566a\u58f0\u3002\nalpha_ = 50.\nnoise = stats.norm.rvs(loc=0, scale=1. / np.sqrt(alpha_), size=n_samples)\n# \u4ea7\u751f \u76ee\u6807\u503c\ny = np.dot(X, w) + noise\n\n# #############################################################################\n# \u62df\u5408 \u8d1d\u53f6\u65af\u5cad\u56de\u5f52\u6a21\u578b \u548c \u6700\u5c0f\u4e8c\u4e58\u6a21\u578b\u3000\u7528\u4e8e\u6bd4\u8f83\nclf = BayesianRidge(compute_score=True)\nclf.fit(X, y)\n\nols = LinearRegression()\nols.fit(X, y)\n\n# #############################################################################\n# \u753b\u51fa \u771f\u6b63\u7684\u6743\u91cd, \u4f30\u8ba1\u51fa\u7684\u6743\u91cd, \u6743\u91cd\u7684\u76f4\u65b9\u56fe\uff0c\u548c\u3000\u4f34\u6709\u6807\u51c6\u504f\u5dee\u7684\u9884\u6d4b\nlw = 2\nplt.figure(figsize=(6, 5))\nplt.title(\"Weights of the model\")\nplt.plot(clf.coef_, color='lightgreen', linewidth=lw,\n         label=\"Bayesian Ridge estimate\")\nplt.plot(w, color='gold', linewidth=lw, label=\"Ground truth\")\nplt.plot(ols.coef_, color='navy', linestyle='--', label=\"OLS estimate\")\nplt.xlabel(\"Features\")\nplt.ylabel(\"Values of the weights\")\nplt.legend(loc=\"best\", prop=dict(size=12))\n\nplt.figure(figsize=(6, 5))\nplt.title(\"Histogram of the weights\")\nplt.hist(clf.coef_, bins=n_features, color='gold', log=True,\n         edgecolor='black')\nplt.scatter(clf.coef_[relevant_features], np.full(len(relevant_features), 5.),\n            color='navy', label=\"Relevant features\")\nplt.ylabel(\"Features\")\nplt.xlabel(\"Values of the weights\")\nplt.legend(loc=\"upper left\")\n\nplt.figure(figsize=(6, 5))\nplt.title(\"Marginal log-likelihood\")\nplt.plot(clf.scores_, color='navy', linewidth=lw)\nplt.ylabel(\"Score\")\nplt.xlabel(\"Iterations\")\n\n\n# \u7ed8\u5236\u4e00\u4e9b\u3000\u591a\u9879\u5f0f\u56de\u5f52\u7684\u9884\u6d4b\ndef f(x, noise_amount):\n    y = np.sqrt(x) * np.sin(x)\n    noise = np.random.normal(0, 1, len(x))\n    return y + noise_amount * noise\n\n\ndegree = 10\nX = np.linspace(0, 10, 100)\ny = f(X, noise_amount=0.1)\nclf_poly = BayesianRidge()\nclf_poly.fit(np.vander(X, degree), y)\n\nX_plot = np.linspace(0, 11, 25)\ny_plot = f(X_plot, noise_amount=0)\ny_mean, y_std = clf_poly.predict(np.vander(X_plot, degree), return_std=True)\nplt.figure(figsize=(6, 5))\nplt.errorbar(X_plot, y_mean, y_std, color='navy',\n             label=\"Polynomial Bayesian Ridge Regression\", linewidth=lw)\nplt.plot(X_plot, y_plot, color='gold', linewidth=lw,\n         label=\"Ground Truth\")\nplt.ylabel(\"Output y\")\nplt.xlabel(\"Feature X\")\nplt.legend(loc=\"lower left\")\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}